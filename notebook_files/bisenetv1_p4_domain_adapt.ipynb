{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxlodwhG7hEE"
      },
      "source": [
        "## **Installing packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sINdgFRi71jI",
        "outputId": "5e0a9cc6-5ec1-47bf-c532-61346b4e6a1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio torchsummary\n",
        "!pip install 'tqdm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dIkbogrr3LY",
        "outputId": "745a0dc8-2007-40e4-8a17-cef98ed9525b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m92.2/101.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JnWbLMOA_v",
        "outputId": "fe7942cc-1ae1-4de9-cf4d-011cceb3b942"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.2)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.10.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=1f09ba10d679d87b23bb84d6a47b173495761b40dae29e6c7ad886ef177f96a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=a2cc2d740468e68e7c08c53a1e7c3e5b76a82ea485d38005676974c7c2851f98\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_JhI28r_mH"
      },
      "source": [
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xBUFnC9vr-FZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "from torch.backends import cudnn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "from torchsummary import summary\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Ycn47MtFEj"
      },
      "source": [
        "## **Import**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXl1I5e7bJS"
      },
      "source": [
        "### Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIh7yVFKOlXF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/Drive')\n",
        "\n",
        "cityscapes = True\n",
        "gta5 = True\n",
        "\n",
        "if not os.path.isdir(f'/content/Cityscapes') and cityscapes:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/Cityscapes.zip\"\n",
        "if not os.path.isdir(f'/content/GTA5') and gta5:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/GTA5.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcgibpX4Cgox"
      },
      "source": [
        "### Import BiSeNet v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isbOyWTSCkuv"
      },
      "outputs": [],
      "source": [
        "# cloning github repo for model (BiseNet with STDC) and utils, I rewrote manually the Train.py and Cityscapes.py below\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "\n",
        "!git clone https://github.com/taveraantonio/RTDA.git\n",
        "from RTDA.models.bisenetv1 import BiSeNet as BiSeNetV1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcrun2fMCJwQ"
      },
      "source": [
        "#### Caltech 101\n",
        "Useful for update weights for BiSeNet v1 training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsnWOD0qB_o0"
      },
      "outputs": [],
      "source": [
        "# Clone github repository with data\n",
        "import pathlib\n",
        "print(pathlib.Path().resolve())\n",
        "if not os.path.isdir('./Caltech101'):\n",
        "  !git clone https://github.com/MachineLearning2020/Homework2-Caltech101.git\n",
        "  !mv 'Homework2-Caltech101' 'Caltech101'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzUXQKPmJEI1"
      },
      "source": [
        "### Import methods from TA's repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CNz4CFTuY6a"
      },
      "outputs": [],
      "source": [
        "# cloning github repo for model (BiseNet with STDC) and utils, I rewrote manually the Train.py and Cityscapes.py below\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/ClaudiaCuttano/AML_Semantic_DA.git\n",
        "\n",
        "\n",
        "# importing stuff from the repo we just cloned\n",
        "\n",
        "# copied from train.py\n",
        "from AML_Semantic_DA.model.model_stages import BiSeNet  # see https://github.com/ClaudiaCuttano/AML_Semantic_DA/blob/master/model/model_stages.py\n",
        "from AML_Semantic_DA.utils import poly_lr_scheduler, reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu # see https://github.com/ClaudiaCuttano/AML_Semantic_DA/blob/master/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LcbPuK6FXza"
      },
      "outputs": [],
      "source": [
        "# taken from the original train.py, not sure what it's for\n",
        "def str2bool(v):\n",
        "    \"\"\"\n",
        "    It converts a string into a boolean\n",
        "    \"\"\"\n",
        "\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise 'Unsupported value encountered.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHU8LM2IeA9T"
      },
      "source": [
        "### Import methods for GTA5\n",
        "Clone repo useful to give a label for each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XS3OpszpeCFq"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/MichaelFan01/STDC-Seg.git STDC_seg\n",
        "# importing stuff from the repo we just cloned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1cLfJGfmsb"
      },
      "source": [
        "## **Data loader classes + Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdWc-Bs6tSBG"
      },
      "source": [
        "### Data Pre Processing\n",
        "We need to do pre-processing only on **training set** (and not on validation set because on the last one we work with 1024 x 2048 pictures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg5N6jHd5dCy"
      },
      "outputs": [],
      "source": [
        "#transforms.CenterCrop((512,1024))\n",
        "train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "eval_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "gta_train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                          transforms.ToTensor()\n",
        "                                          ])\n",
        "gta_label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "gta_val_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "#data augmentation\n",
        "bright_t = transforms.ColorJitter(brightness=[1,2])\n",
        "contrast_t = transforms.ColorJitter(contrast = [2,5])\n",
        "saturation_t = transforms.ColorJitter(saturation = [1,3])\n",
        "hue_t = transforms.ColorJitter(hue = 0.2)\n",
        "gs_t = transforms.Grayscale(3)\n",
        "hflip_t = transforms.RandomHorizontalFlip(p = 1)\n",
        "cc_t = transforms.CenterCrop((256,512))\n",
        "\n",
        "augmentation_transforms = [bright_t, contrast_t, saturation_t, hue_t, gs_t, hflip_t, cc_t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcJLDmjscHZ"
      },
      "source": [
        "### Cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrzZRbQsFeJq"
      },
      "outputs": [],
      "source": [
        "class CityScapes(Dataset):\n",
        "    def __init__(self, base_root, mode):\n",
        "        super(CityScapes, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.mask_paths_colored = [] # colored images\n",
        "        self.mask_paths_bw = [] # labels\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "        image_folder = f'{base_root}images/{mode}'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                mask_path_bw = image_path.replace('leftImg8bit', 'gtFine_labelTrainIds')\n",
        "                mask_path_bw = mask_path_bw.replace('/images/', '/gtFine/')\n",
        "                assert(Path(mask_path_bw).is_file())\n",
        "                self.mask_paths_bw.append(mask_path_bw)\n",
        "\n",
        "                #mask_path_colored = image_path.replace('leftImg8bit', 'gtFine_labelTrainIds')\n",
        "                #mask_path_colored = mask_path_colored.replace('/images/', '/gtFine/')\n",
        "                #assert(Path(mask_path_colored).is_file())\n",
        "                #self.mask_paths_colored.append(mask_path_colored)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.image_transform = train_transform\n",
        "        if self.mode == 'val':\n",
        "            self.image_transform = eval_transform\n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.mask_paths_bw))\n",
        "        #assert (len(self.image_paths) == len(self.mask_paths_colored))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        label = Image.open(self.mask_paths_bw[index])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            label = np.array(self.label_transform(label))[np.newaxis, :]\n",
        "        else:\n",
        "            label = np.array(label)[np.newaxis, :]\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcCX6cTyyBi"
      },
      "source": [
        "### GTA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2VtAICsy0QY"
      },
      "outputs": [],
      "source": [
        "class GTA5(Dataset):\n",
        "    def __init__(self, base_root, mode, augmentation=False, train_test_rateo=2/3):\n",
        "        super(GTA5, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.label_paths = [] # labels\n",
        "        self.train_test_rateo = train_test_rateo\n",
        "        self.augmentation = augmentation\n",
        "        with open('STDC_seg/cityscapes_info.json', 'r') as fr:\n",
        "            labels_info = json.load(fr)\n",
        "        self.label_map = {el['id']: el['trainId'] for el in labels_info}\n",
        "        self.label_map.update({34 : 255})\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "\n",
        "        image_folder = f'{base_root}images'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                label_path = image_path.replace('/images/', '/labels/')\n",
        "                assert(Path(label_path).is_file())\n",
        "                self.label_paths.append(label_path)\n",
        "\n",
        "        l = int(len(self.image_paths) * self.train_test_rateo)\n",
        "        if self.mode == 'train':\n",
        "            self.image_paths = self.image_paths[:l]\n",
        "            self.label_paths = self.label_paths[:l]\n",
        "            self.image_transform = gta_train_transform\n",
        "            self.label_transform = gta_label_transform\n",
        "        elif self.mode == 'val':\n",
        "            self.image_paths = self.image_paths[l:]\n",
        "            self.label_paths = self.label_paths[l:]\n",
        "            self.image_transform = gta_val_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.label_paths))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        label = Image.open(self.label_paths[index])\n",
        "        if self.mode == 'train':\n",
        "            label = self.label_transform(label)\n",
        "\n",
        "        if self.augmentation and random.choice([True, False]) and self.mode == 'train':\n",
        "            idx = random.randint(0, 6)\n",
        "\n",
        "            image = augmentation_transforms[idx](image)\n",
        "\n",
        "            if hflip_t is augmentation_transforms[idx] or cc_t is augmentation_transforms[idx]:\n",
        "                label = augmentation_transforms[idx](label)\n",
        "\n",
        "            if cc_t is augmentation_transforms[idx]:\n",
        "                rimage_t = transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR, antialias=None)\n",
        "                rlabel_t = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST, antialias=None)\n",
        "                image = rimage_t(image)\n",
        "                label = augmentation_transforms[idx](label)\n",
        "                label = rlabel_t(label)\n",
        "\n",
        "        label = np.array(label).astype(np.int64)[np.newaxis, :]\n",
        "        label = self.convert_labels(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def convert_labels(self, label):\n",
        "        for k, v in self.label_map.items():\n",
        "            label[label == k] = v\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFKadbbIGdzN"
      },
      "source": [
        "## **Architecture**\n",
        "BiSeNet as Domain Adaptation Neural Network (DANN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORORR6UuGmkv"
      },
      "outputs": [],
      "source": [
        "class DepthWiseSeparableConvolution(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(DepthWiseSeparableConvolution, self).__init__()\n",
        "        self.depth_wise = nn.Conv2d(ch_in, ch_in, kernel_size=4, stride=2, padding=1, groups=ch_in)\n",
        "        self.point_wise = nn.Conv2d(ch_in, ch_out, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depth_wise(x)\n",
        "        out = self.point_wise(out)\n",
        "        return out\n",
        "\n",
        "class LightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.conv3 = DepthWiseSeparableConvolution(ndf*2, ndf*4)\n",
        "        self.conv4 = DepthWiseSeparableConvolution(ndf*4, ndf*8)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*8, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "    \n",
        "class LightLightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightLightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*2, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class FCDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, ndf = 64):\n",
        "        super(FCDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        #x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIHcQMaMG_o9"
      },
      "source": [
        "## **Train + Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_IPp-nNYxDe"
      },
      "source": [
        "### Base Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0DE8M0pFGhy"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train(args, model, optimizer, dataloader_train, dataloader_val):\n",
        "    print(\"start train without domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/AML project/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch, max_iter=args.num_epochs)\n",
        "        model.train() # Sets module in training mode\n",
        "        tq = tqdm(total=len(dataloader_train) * args.batch_size)\n",
        "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
        "        loss_record = []\n",
        "\n",
        "        for i, (data, label) in enumerate(dataloader_train):\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "            # From: [batch_size, height, width, channels]\n",
        "            # To: [batch_size, channels, height, width]\n",
        "            #data = data.permute(0, 3, 1, 2)\n",
        "\n",
        "            with amp.autocast():\n",
        "                output, out16, out32 = model(data)  # Forward pass to the network\n",
        "                # Compute loss based on output and ground truth\n",
        "                loss1 = loss_func(output, label.squeeze(1))\n",
        "                loss2 = loss_func(out16, label.squeeze(1))\n",
        "                loss3 = loss_func(out32, label.squeeze(1))\n",
        "                loss = loss1 + loss2 + loss3  # sum of losses\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            scaler.scale(loss).backward() # backward pass: computes gradients\n",
        "            scaler.step(optimizer)        # update weights based on accumulated gradients\n",
        "            scaler.update()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            tq.set_postfix(loss='%.6f' % loss)\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_step', loss, step)\n",
        "            loss_record.append(loss.item())\n",
        "        tq.close()\n",
        "        loss_train_mean = np.mean(loss_record)\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
        "        print('loss for train : %f' % (loss_train_mean))\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KShRQ9Y4Gb"
      },
      "source": [
        "### Domain Adaptation Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSs7A9BOXTDS"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=False, dataloader_target=None):\n",
        "    print(\"start train with domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/AML project/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    loss_func_G = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    loss_func_adv = torch.nn.BCEWithLogitsLoss()\n",
        "    loss_func_D = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    # see (https://www.github.com/wasidennis/AdaptSegNet/blob/master/train_gta2cityscapes_multi)\n",
        "    print(\"Train DA\")\n",
        "\n",
        "    LAMBDA_ADV_TARGET = 0.001\n",
        "\n",
        "    dataloader_len = min(len(dataloader_train), len(dataloader_target))\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch-1, max_iter=args.num_epochs)\n",
        "        lr_D = poly_lr_scheduler(optimizer_D, args.learning_rate_D, iter=epoch-1, max_iter=args.num_epochs)\n",
        "\n",
        "        model.train()\n",
        "        model_D.train()\n",
        "\n",
        "        tq = tqdm(total=dataloader_len * args.batch_size)\n",
        "\n",
        "        tq.set_description('epoch %d, lr %f, lr_discriminator %f' % (epoch, lr, lr_D))\n",
        "\n",
        "        # set the ground truth for the discriminator\n",
        "        source_label = 0\n",
        "        target_label = 1\n",
        "        # initiate lists to track the losses\n",
        "        loss_G_record = []                                                       # track the Segmentation loss\n",
        "        loss_adv_record = []                                                     # track the advarsirial loss\n",
        "        loss_D_record = []                                                       # track the discriminator loss\n",
        "\n",
        "        source_train_loader = enumerate(dataloader_train)\n",
        "        s_size = len(dataloader_train)\n",
        "        target_loader = enumerate(dataloader_target)\n",
        "        t_size = len(dataloader_target)\n",
        "\n",
        "        for i in range(dataloader_len):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # =====================================\n",
        "            # train Generator G:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(source_train_loader)\n",
        "            data, label = batch\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "\n",
        "            with amp.autocast():\n",
        "                output_s, out16, out32 = model(data)\n",
        "                loss1 = loss_func_G(output_s, label.squeeze(1))\n",
        "                loss2 = loss_func_G(out16, label.squeeze(1))\n",
        "                loss3 = loss_func_G(out32, label.squeeze(1))\n",
        "                loss_G = loss1 + loss2 + loss3\n",
        "\n",
        "            scaler.scale(loss_G).backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(target_loader)\n",
        "\n",
        "            data, _ = batch\n",
        "            data = data.cuda()\n",
        "            with amp.autocast():\n",
        "\n",
        "                output_t, _, _ = model(data)\n",
        "                D_out = model_D(F.softmax(output_t))\n",
        "                loss_adv = loss_func_adv(D_out , Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda() )  # Generator try to fool the discriminator\n",
        "                loss_adv = loss_adv * LAMBDA_ADV_TARGET\n",
        "\n",
        "            scaler.scale(loss_adv).backward()\n",
        "\n",
        "            # =====================================\n",
        "            # train Discriminator D:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            output_s = output_s.detach()\n",
        "            with amp.autocast():\n",
        "                D_out = model_D(F.softmax(output_s))                                                                   # we feed the discriminator with the output of the G-model\n",
        "                loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda())\n",
        "                loss_D = loss_D / 2\n",
        "            scaler.scale(loss_D).backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            output_t = output_t.detach()\n",
        "            with amp.autocast():\n",
        "                D_out = model_D(F.softmax(output_t))  # we feed the discriminator with the output of the model\n",
        "                loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(target_label)).cuda())  # add the adversarial loss\n",
        "                loss_D = loss_D / 2\n",
        "            scaler.scale(loss_D).backward()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            losses = {\"loss_seg\" : '%.6f' %(loss_G.item())  , \"loss_adv\" : '%.6f' %(loss_adv.item()) , \"loss_D\" : '%.6f'%(loss_D.item()) } # add dictionary to print losses\n",
        "            tq.set_postfix(losses)\n",
        "\n",
        "            loss_G_record.append(loss_G.item())\n",
        "            loss_adv_record.append(loss_adv.item())\n",
        "            loss_D_record.append(loss_D.item())\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_G_step', loss_G, step)  # track the segmentation loss\n",
        "            writer.add_scalar('loss_adv_step', loss_adv, step)  # track the adversarial loss\n",
        "            writer.add_scalar('loss_D_step', loss_D, step)  # track the discreminator loss\n",
        "            scaler.step(optimizer)  # update the optimizer for genarator\n",
        "            scaler.step(optimizer_D)  # update the optimizer for discriminator\n",
        "            scaler.update()\n",
        "        tq.close()\n",
        "\n",
        "        loss_seg_record_mean = np.mean(loss_G_record)\n",
        "        loss_adv_record_mean = np.mean(loss_adv_record)\n",
        "        loss_D_record_mean = np.mean(loss_D_record)\n",
        "        loss_mean = np.mean([loss_seg_record_mean, loss_adv_record_mean, loss_D_record_mean])\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_mean), epoch)\n",
        "        print(f'loss for train :\\n - Segmentation: {loss_seg_record_mean}\\n - Adversarial: {loss_adv_record_mean}\\n - Discriminator: {loss_D_record_mean}')\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}BSv1_Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}BSv1_Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM5ZhKsY83S"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYVQs2j2FGpx"
      },
      "outputs": [],
      "source": [
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        tq = tqdm(total=len(dataloader))\n",
        "        tq.set_description('Validation')\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            label = label.type(torch.LongTensor)\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "\n",
        "            # get RGB predict image\n",
        "            predict = model(data)\n",
        "            predict = predict.squeeze(0)\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            # there is no need to transform the one-hot array to visual RGB array\n",
        "            # predict = colour_code_segmentation(np.array(predict), label_info)\n",
        "            # label = colour_code_segmentation(np.array(label), label_info)\n",
        "            precision_record.append(precision)\n",
        "\n",
        "            tq.update(1)\n",
        "\n",
        "        tq.close()\n",
        "        precision = np.mean(precision_record)\n",
        "        miou_list = per_class_iu(hist)\n",
        "        miou = np.mean(miou_list)\n",
        "        print('\\nprecision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        print(f'mIoU per class: {miou_list}')\n",
        "\n",
        "        return precision, miou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-F6RptJg00d"
      },
      "source": [
        "## **Plots**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u7wT0h2sabK"
      },
      "source": [
        "### Loss plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRgbVCDEpWBG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "tr = [x[0] for x in loss_list]\n",
        "vl = [x[1] for x in loss_list]\n",
        "\n",
        "print(tr)\n",
        "print(vl)\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Loss comparison\")\n",
        "plt.plot(list(range(1, NUM_EPOCHS + 1, 1)), tr, label = 'training loss')\n",
        "plt.plot(list(range(1, NUM_EPOCHS + 1, 1)), vl, label = 'validation loss')\n",
        "plt.xlabel(\"# epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.savefig(PLOTS + f'/loss_{OPTIM}_{np.log10(LR):.0f}_{NUM_EPOCHS}_{SCHEDULER}.png')\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMo6X2xxsjXf"
      },
      "source": [
        "### Accuracy (mIOU) plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FiUQfn9sq_B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "tr = [x[0] for x in acc_list]\n",
        "vl = [x[1] for x in acc_list]\n",
        "\n",
        "plt.figure()\n",
        "plt.title(\"Accuracy comparison\")\n",
        "plt.plot(list(range(1, NUM_EPOCHS + 1, 1)), tr, label = 'training accuracy')\n",
        "plt.plot(list(range(1, NUM_EPOCHS + 1, 1)), vl, label = 'validation accuracy')\n",
        "plt.xlabel(\"# epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(PLOTS + f'/accuracy_{OPTIM}_{np.log10(LR):.0f}_{NUM_EPOCHS}_{SCHEDULER}.png')\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxekmR745ySe"
      },
      "source": [
        "## **Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSHcUqLB5yWO"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
        "net.train(False) # Set Network to evaluation mode\n",
        "\n",
        "# comando duale per caricare la rete migliore in net dal file .pth\n",
        "# per recuparlo --> load_state_dict()\n",
        "# torch.nn.Module.load_state_dict() -> it loads a model’s parameter dictionary using a deserialized state_dict. (visit https://pytorch.org/tutorials/beginner/saving_loading_models.html)\n",
        "\n",
        "running_corrects = 0\n",
        "for images, labels in tqdm(test_dataloader):\n",
        "  images = images.to(DEVICE)\n",
        "  labels = labels.to(DEVICE)\n",
        "\n",
        "  # Forward Pass\n",
        "  outputs = net(images)\n",
        "\n",
        "  # Get predictions\n",
        "  _, preds = torch.max(outputs.data, 1)\n",
        "\n",
        "  # Update Corrects\n",
        "  running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = running_corrects / float(len(test_dataset))\n",
        "\n",
        "print('Test Accuracy: {}'.format(accuracy))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJR33K6HdBA"
      },
      "source": [
        "## **Main**\n",
        "Prepare arguments, Datasets, Dataloaders, model, training and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mND4TmxJ2b1v"
      },
      "outputs": [],
      "source": [
        "def main(args, eval_only=False):\n",
        "\n",
        "    n_classes = args.num_classes\n",
        "    train_root = args.train_root\n",
        "    val_root = args.val_root\n",
        "    target_root = args.train_root\n",
        "    domain_adapt = args.domain_adaptation\n",
        "\n",
        "    # defining the training, validation (and target for domain adaptation) datasets and dataloaders\n",
        "    if train_root == 'GTA5/':\n",
        "        if train_root != val_root: # if we only use GTA5 for training, use the entire dataset and don't leave a portion for testing\n",
        "            train_dataset = GTA5(train_root, 'train', args.training_augmentation, 1)\n",
        "        else:\n",
        "            train_dataset = GTA5(train_root, 'train', args.training_augmentation)\n",
        "    else:\n",
        "        train_dataset = CityScapes(train_root, 'train')\n",
        "\n",
        "    if val_root == 'GTA5/':\n",
        "        val_dataset = GTA5(val_root, 'val')\n",
        "    else:\n",
        "        val_dataset = CityScapes(val_root, 'val')\n",
        "\n",
        "    if target_root == 'GTA5/':\n",
        "        target_dataset = GTA5(target_root, 'train')\n",
        "    else:\n",
        "        target_dataset = CityScapes(target_root, 'train')\n",
        "\n",
        "    dataloader_train = DataLoader(train_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    dataloader_val = DataLoader(val_dataset,\n",
        "                      batch_size=1,\n",
        "                      shuffle=False,\n",
        "                      num_workers=args.num_workers,\n",
        "                      drop_last=False)\n",
        "\n",
        "    dataloader_target = DataLoader(target_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    ## model\n",
        "    model = BiSeNetV1(num_classes=n_classes, context_path='resnet18')\n",
        "\n",
        "    if args.epoch_start_i != 0:\n",
        "        print(f'loading data from saved model {args.saved_model}')\n",
        "        model.load_state_dict(torch.load(f'{args.save_model_path}{args.saved_model}'))\n",
        "\n",
        "    if torch.cuda.is_available() and args.use_gpu:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    print(\"\\t\\tModel:\\n\")\n",
        "    summary(model, (3, 512, 1024), batch_size=-1, device='cuda')\n",
        "\n",
        "    ## optimizer\n",
        "    # build optimizer\n",
        "    if args.optimizer == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "    elif args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "    else:  # rmsprop\n",
        "        print('not supported optimizer \\n')\n",
        "        return None\n",
        "\n",
        "    if domain_adapt:\n",
        "        # init Discriminator\n",
        "        model_D = LightFCDiscriminator(args.num_classes)\n",
        "\n",
        "        model_D = model_D.cuda()\n",
        "\n",
        "        if args.optimizer_D == 'rmsprop':\n",
        "            optimizer_D = torch.optim.RMSprop(model_D.parameters(), args.learning_rate_D)\n",
        "        elif args.optimizer_D == 'sgd':\n",
        "            optimizer_D = torch.optim.SGD(model_D.parameters(), args.learning_rate_D, momentum=0.9, weight_decay=1e-4)\n",
        "        elif args.optimizer_D == 'adam':\n",
        "            optimizer_D = torch.optim.Adam(model_D.parameters(), args.learning_rate_D, betas=(0.9, 0.99))\n",
        "        else:  # rmsprop\n",
        "            print('not supported optimizer \\n')\n",
        "            return None\n",
        "\n",
        "    if not eval_only: #this is for when we only care about evaluating a saved model and not about training\n",
        "        if domain_adapt:\n",
        "            print(\"\\t\\tDiscriminator:\\n\")\n",
        "            summary(model_D, (19, 512, 1024), batch_size=-1, device='cuda')\n",
        "            ## train loop for domain adaptation\n",
        "            train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=domain_adapt, dataloader_target=dataloader_target)\n",
        "        else:\n",
        "            ## normal train loop\n",
        "            train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "    # final test\n",
        "    val(args, model, dataloader_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvA6e8s5HXFJ"
      },
      "source": [
        "### main execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ph6kWFEQFkQS"
      },
      "outputs": [],
      "source": [
        "class arguments():\n",
        "    mode = 'train'\n",
        "    backbone = 'CatmodelSmall'\n",
        "    pretrain_path = \"/content/Drive/MyDrive/Colab Notebooks/checkpoints/STDCNet813M_73.91.tar\"\n",
        "    use_conv_last = False\n",
        "    num_epochs = 50\n",
        "    epoch_start_i = 0\n",
        "    checkpoint_step = 1\n",
        "    validation_step = 5\n",
        "    crop_height = 512\n",
        "    crop_width = 1024\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-3\n",
        "    learning_rate_D = 1e-4\n",
        "    num_workers = 2\n",
        "    num_classes = 19\n",
        "    cuda = '0'\n",
        "    use_gpu = True\n",
        "    save_model_path = '/content/Drive/MyDrive/Colab Notebooks/Partial models/'\n",
        "    saved_model = f'BSv1_Saved_model_epoch_{epoch_start_i}.pth'\n",
        "    optimizer = 'adam'\n",
        "    optimizer_D = 'adam'\n",
        "    loss = 'crossentropy'\n",
        "    #'Cityscapes/Cityspaces/'\n",
        "    #'GTA5/'\n",
        "    train_root = 'GTA5/'\n",
        "    val_root = 'Cityscapes/Cityspaces/'\n",
        "    target_root = 'Cityscapes/Cityspaces/'\n",
        "    training_augmentation = True\n",
        "    domain_adaptation = True\n",
        "main_args = arguments()\n",
        "\n",
        "main(main_args, eval_only=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "OXcCX6cTyyBi",
        "d-F6RptJg00d",
        "jxekmR745ySe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
