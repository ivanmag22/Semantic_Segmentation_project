{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxlodwhG7hEE"
      },
      "source": [
        "## **Installing packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sINdgFRi71jI",
        "outputId": "0953b34d-76cf-403e-b1a2-a0c11badd8a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio torchsummary\n",
        "!pip install 'tqdm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dIkbogrr3LY",
        "outputId": "e8db7667-f226-4d70-bebb-8343a03186cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JnWbLMOA_v",
        "outputId": "1e6f333d-f0b8-4d6e-d443-6db4390ae41e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.11.0)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=edeffede6ad62db95bc7d63964dda616df913ef9e00e428912509ab03575bea9\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=b6e735525474c7d2d952c525f6fa04dd36dff445fadd5186c8f2f875e8ddb303\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkRa9GIj7GZQ",
        "outputId": "e8e9d8a0-d871-4014-a212-b31d955d8389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->thop) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        }
      ],
      "source": [
        "!pip install thop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_JhI28r_mH"
      },
      "source": [
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBUFnC9vr-FZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "from torch.backends import cudnn\n",
        "import torch.cuda.amp as amp\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "from torchsummary import summary\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from thop import profile\n",
        "\n",
        "\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Ycn47MtFEj"
      },
      "source": [
        "## **Import**\n",
        "To import dataset and pretrained model you should add a shortcut to the shared folders in \"Google Drive/MyDrive/Colab Notebooks\". We have tried also with another command that the assistant suggested, but it does not work well.\n",
        "\n",
        "We have imported stuff from our own repository, but also from other repositories (TA's one and one suggested by her)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXl1I5e7bJS"
      },
      "source": [
        "### Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIh7yVFKOlXF",
        "outputId": "d5d88e08-e729-46e3-a7d6-581af9329f56"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/Drive')\n",
        "\n",
        "cityscapes = True\n",
        "gta5 = True\n",
        "\n",
        "if not os.path.isdir(f'/content/Cityscapes') and cityscapes:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/Cityscapes.zip\"\n",
        "if not os.path.isdir(f'/content/GTA5') and gta5:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/GTA5.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzUXQKPmJEI1"
      },
      "source": [
        "### Import methods from TA's repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CNz4CFTuY6a",
        "outputId": "3a3b81c0-30e5-4b73-d131-cb58cb10c8cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'AML_Semantic_DA'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 10\u001b[K\n",
            "Receiving objects: 100% (18/18), 11.20 KiB | 11.20 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# cloning github repo for model (STDC-net) and utils, we rewrote manually the Train.py and Cityscapes.py below\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/ClaudiaCuttano/AML_Semantic_DA.git\n",
        "\n",
        "\n",
        "# importing stuff from the repo we just cloned\n",
        "\n",
        "from AML_Semantic_DA.model.model_stages import BiSeNet\n",
        "from AML_Semantic_DA.utils import poly_lr_scheduler, reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYmppeDwAqNY"
      },
      "source": [
        "### Import BiSeNet v1\n",
        "Clone our GitHub repo: https://github.com/ivanmag22/Semantic_Segmentation_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lh2kJl2AqNf",
        "outputId": "95bea602-ee7e-417c-b651-7b3926d958fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'Semantic_Segmentation_project'...\n",
            "remote: Enumerating objects: 70, done.\u001b[K\n",
            "remote: Total 70 (delta 0), reused 0 (delta 0), pack-reused 70\u001b[K\n",
            "Receiving objects: 100% (70/70), 88.83 MiB | 32.64 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ],
      "source": [
        "# cloning github repo for model (BiseNet v1) and utils\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/ivanmag22/Semantic_Segmentation_project.git\n",
        "\n",
        "\n",
        "# importing stuff from the repo we just cloned\n",
        "\n",
        "from Semantic_Segmentation_project.model.bisenetv1 import BiSeNet as BiSeNetv1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHU8LM2IeA9T"
      },
      "source": [
        "### Import methods for GTA5\n",
        "Clone repo useful to give a label for each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS3OpszpeCFq",
        "outputId": "506ae8c0-3ea7-48ab-df0f-edeef90606df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'STDC_seg'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 98 (delta 18), reused 8 (delta 8), pack-reused 70\u001b[K\n",
            "Receiving objects: 100% (98/98), 1.93 MiB | 3.53 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/MichaelFan01/STDC-Seg.git STDC_seg\n",
        "# importing stuff from the repo we just cloned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1cLfJGfmsb"
      },
      "source": [
        "## **Data loader classes + Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdWc-Bs6tSBG"
      },
      "source": [
        "### Data Pre Processing\n",
        "We need to do pre-processing only on **training set** (and not on validation set because on the last one we work with 1024 x 2048 pictures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg5N6jHd5dCy"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "eval_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "gta_train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                          transforms.ToTensor()\n",
        "                                          ])\n",
        "gta_label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "gta_val_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "#data augmentation\n",
        "bright_t = transforms.ColorJitter(brightness=[1,2])\n",
        "contrast_t = transforms.ColorJitter(contrast = [2,5])\n",
        "saturation_t = transforms.ColorJitter(saturation = [1,3])\n",
        "hue_t = transforms.ColorJitter(hue = 0.2)\n",
        "gs_t = transforms.Grayscale(3)\n",
        "hflip_t = transforms.RandomHorizontalFlip(p = 1)\n",
        "cc_t = transforms.CenterCrop((256,512))\n",
        "\n",
        "augmentation_transforms = [bright_t, contrast_t, saturation_t, hue_t, gs_t, hflip_t, cc_t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcJLDmjscHZ"
      },
      "source": [
        "### Cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrzZRbQsFeJq"
      },
      "outputs": [],
      "source": [
        "class CityScapes(Dataset):\n",
        "    def __init__(self, base_root, mode):\n",
        "        super(CityScapes, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.mask_paths_colored = [] # colored images\n",
        "        self.mask_paths_bw = [] # labels\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "        image_folder = f'{base_root}images/{mode}'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                mask_path_bw = image_path.replace('leftImg8bit', 'gtFine_labelTrainIds')\n",
        "                mask_path_bw = mask_path_bw.replace('/images/', '/gtFine/')\n",
        "                assert(Path(mask_path_bw).is_file())\n",
        "                self.mask_paths_bw.append(mask_path_bw)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.image_transform = train_transform\n",
        "        if self.mode == 'val':\n",
        "            self.image_transform = eval_transform\n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.mask_paths_bw))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        label = Image.open(self.mask_paths_bw[index])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            label = np.array(self.label_transform(label))[np.newaxis, :]\n",
        "        else:\n",
        "            label = np.array(label)[np.newaxis, :]\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcCX6cTyyBi"
      },
      "source": [
        "### GTA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2VtAICsy0QY"
      },
      "outputs": [],
      "source": [
        "class GTA5(Dataset):\n",
        "    def __init__(self, base_root, mode, augmentation=False, train_test_rateo=2/3):\n",
        "        super(GTA5, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.label_paths = [] # labels\n",
        "        self.train_test_rateo = train_test_rateo\n",
        "        self.augmentation = augmentation\n",
        "        with open('STDC_seg/cityscapes_info.json', 'r') as fr:\n",
        "            labels_info = json.load(fr)\n",
        "        self.label_map = {el['id']: el['trainId'] for el in labels_info}\n",
        "        self.label_map.update({34 : 255})\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "\n",
        "        image_folder = f'{base_root}images'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                label_path = image_path.replace('/images/', '/labels/')\n",
        "                assert(Path(label_path).is_file())\n",
        "                self.label_paths.append(label_path)\n",
        "\n",
        "        l = int(len(self.image_paths) * self.train_test_rateo)\n",
        "        if self.mode == 'train':\n",
        "            self.image_paths = self.image_paths[:l]\n",
        "            self.label_paths = self.label_paths[:l]\n",
        "            self.image_transform = gta_train_transform\n",
        "            self.label_transform = gta_label_transform\n",
        "        elif self.mode == 'val':\n",
        "            self.image_paths = self.image_paths[l:]\n",
        "            self.label_paths = self.label_paths[l:]\n",
        "            self.image_transform = gta_val_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.label_paths))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        label = Image.open(self.label_paths[index])\n",
        "        if self.mode == 'train':\n",
        "            label = self.label_transform(label)\n",
        "\n",
        "        if self.augmentation and random.choice([True, False]) and self.mode == 'train':\n",
        "            idx = random.randint(0, 6)\n",
        "\n",
        "            image = augmentation_transforms[idx](image)\n",
        "\n",
        "            if hflip_t is augmentation_transforms[idx] or cc_t is augmentation_transforms[idx]:\n",
        "                label = augmentation_transforms[idx](label)\n",
        "\n",
        "            if cc_t is augmentation_transforms[idx]:\n",
        "                rimage_t = transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR, antialias=None)\n",
        "                rlabel_t = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST, antialias=None)\n",
        "                image = rimage_t(image)\n",
        "                label = augmentation_transforms[idx](label)\n",
        "                label = rlabel_t(label)\n",
        "\n",
        "        label = np.array(label).astype(np.int64)[np.newaxis, :]\n",
        "        label = self.convert_labels(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def convert_labels(self, label):\n",
        "        for k, v in self.label_map.items():\n",
        "            label[label == k] = v\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFKadbbIGdzN"
      },
      "source": [
        "## **Discriminators**\n",
        "- *FCDiscriminator*: standard discriminator\n",
        "- *LightFCDiscriminator*: discriminator with depth pointwise convolutional layers\n",
        "- *LightLightFCDiscriminator*: like the previous discriminator, but with three convolutional layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORORR6UuGmkv"
      },
      "outputs": [],
      "source": [
        "class DepthWiseSeparableConvolution(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(DepthWiseSeparableConvolution, self).__init__()\n",
        "        self.depth_wise = nn.Conv2d(ch_in, ch_in, kernel_size=4, stride=2, padding=1, groups=ch_in)\n",
        "        self.point_wise = nn.Conv2d(ch_in, ch_out, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depth_wise(x)\n",
        "        out = self.point_wise(out)\n",
        "        return out\n",
        "\n",
        "class LightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.conv3 = DepthWiseSeparableConvolution(ndf*2, ndf*4)\n",
        "        self.conv4 = DepthWiseSeparableConvolution(ndf*4, ndf*8)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*8, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class LightLightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightLightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*2, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=8, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class FCDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, ndf = 64):\n",
        "        super(FCDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIHcQMaMG_o9"
      },
      "source": [
        "## **Train + Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_IPp-nNYxDe"
      },
      "source": [
        "### Base Training\n",
        "Same dataset for both training and validation set or different datasets (domain shift)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0DE8M0pFGhy"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train(args, model, optimizer, dataloader_train, dataloader_val):\n",
        "    print(\"start train without domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/Colab Notebooks/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch, max_iter=args.num_epochs)\n",
        "        model.train() # Sets module in training mode\n",
        "        tq = tqdm(total=len(dataloader_train) * args.batch_size)\n",
        "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
        "        loss_record = []\n",
        "\n",
        "        for i, (data, label) in enumerate(dataloader_train):\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "            with amp.autocast():\n",
        "                output, out16, out32 = model(data)  # Forward pass to the network\n",
        "                # Compute loss based on output and ground truth\n",
        "                loss1 = loss_func(output, label.squeeze(1))\n",
        "                loss2 = loss_func(out16, label.squeeze(1))\n",
        "                loss3 = loss_func(out32, label.squeeze(1))\n",
        "                loss = loss1 + loss2 + loss3  # sum of losses\n",
        "\n",
        "            # Compute gradients for each layer and update weights\n",
        "            scaler.scale(loss).backward() # backward pass: computes gradients\n",
        "            scaler.step(optimizer)        # update weights based on accumulated gradients\n",
        "            scaler.update()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            tq.set_postfix(loss='%.6f' % loss)\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_step', loss, step)\n",
        "            loss_record.append(loss.item())\n",
        "        tq.close()\n",
        "        loss_train_mean = np.mean(loss_record)\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
        "        print('loss for train : %f' % (loss_train_mean))\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KShRQ9Y4Gb"
      },
      "source": [
        "### Domain Adaptation Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSs7A9BOXTDS"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=False, dataloader_target=None):\n",
        "    print(\"start train with domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/Colab Notebooks/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    scaler = amp.GradScaler()\n",
        "\n",
        "    loss_func_G = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    loss_func_adv = torch.nn.BCEWithLogitsLoss()\n",
        "    loss_func_D = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    # see (https://www.github.com/wasidennis/AdaptSegNet/blob/master/train_gta2cityscapes_multi.py)\n",
        "\n",
        "    LAMBDA_ADV_TARGET = args.adv_factor\n",
        "\n",
        "    dataloader_len = min(len(dataloader_train), len(dataloader_target))\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch-1, max_iter=args.num_epochs)\n",
        "        lr_D = poly_lr_scheduler(optimizer_D, args.learning_rate_D, iter=epoch-1, max_iter=args.num_epochs)\n",
        "\n",
        "        model.train()\n",
        "        model_D.train()\n",
        "\n",
        "        tq = tqdm(total=dataloader_len * args.batch_size)\n",
        "\n",
        "        tq.set_description('epoch %d, lr %f, lr_discriminator %f' % (epoch, lr, lr_D))\n",
        "\n",
        "        # set the ground truth for the discriminator\n",
        "        source_label = 0\n",
        "        target_label = 1\n",
        "        # initiate lists to track the losses\n",
        "        loss_G_record = []                                                       # track the Segmentation loss\n",
        "        loss_adv_record = []                                                     # track the advarsirial loss\n",
        "        loss_D_record = []                                                       # track the discriminator loss\n",
        "\n",
        "        source_train_loader = enumerate(dataloader_train)\n",
        "        s_size = len(dataloader_train)\n",
        "        target_loader = enumerate(dataloader_target)\n",
        "        t_size = len(dataloader_target)\n",
        "\n",
        "        for i in range(dataloader_len):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # =====================================\n",
        "            # train Generator G:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(source_train_loader)\n",
        "            data, label = batch\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "\n",
        "            with amp.autocast():\n",
        "\n",
        "                output_s, out16, out32 = model(data)\n",
        "                loss1 = loss_func_G(output_s, label.squeeze(1))\n",
        "                loss2 = loss_func_G(out16, label.squeeze(1))\n",
        "                loss3 = loss_func_G(out32, label.squeeze(1))\n",
        "                loss_G = loss1 + loss2 + loss3\n",
        "\n",
        "            scaler.scale(loss_G).backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(target_loader)\n",
        "\n",
        "            data, _ = batch\n",
        "            data = data.cuda()\n",
        "            with amp.autocast():\n",
        "\n",
        "                output_t, _, _ = model(data)\n",
        "\n",
        "                D_out = model_D(F.softmax(output_t, dim=1))\n",
        "                loss_adv = loss_func_adv(D_out , Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda() )  # Generator try to fool the discriminator\n",
        "                loss_adv = loss_adv * LAMBDA_ADV_TARGET\n",
        "\n",
        "            scaler.scale(loss_adv).backward()\n",
        "\n",
        "            # =====================================\n",
        "            # train Discriminator D:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            output_s = output_s.detach()\n",
        "            with amp.autocast():\n",
        "                D_out = model_D(F.softmax(output_s, dim=1))                                                                   # we feed the discriminator with the output of the G-model\n",
        "                loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)).cuda())\n",
        "\n",
        "            scaler.scale(loss_D).backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            output_t = output_t.detach()\n",
        "            with amp.autocast():\n",
        "                D_out = model_D(F.softmax(output_t, dim=1))\n",
        "                loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(target_label)).cuda())\n",
        "\n",
        "            scaler.scale(loss_D).backward()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            losses = {\"loss_seg\" : '%.6f' %(loss_G.item())  , \"loss_adv\" : '%.6f' %(loss_adv.item()) , \"loss_D\" : '%.6f'%(loss_D.item()) } # add dictionary to print losses\n",
        "            tq.set_postfix(losses)\n",
        "\n",
        "            loss_G_record.append(loss_G.item())\n",
        "            loss_adv_record.append(loss_adv.item())\n",
        "            loss_D_record.append(loss_D.item())\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_G_step', loss_G, step)  # track the segmentation loss\n",
        "            writer.add_scalar('loss_adv_step', loss_adv, step)  # track the adversarial loss\n",
        "            writer.add_scalar('loss_D_step', loss_D, step)  # track the discreminator loss\n",
        "            scaler.step(optimizer)  # update the optimizer for genarator\n",
        "            scaler.step(optimizer_D)  # update the optimizer for discriminator\n",
        "            scaler.update()\n",
        "        tq.close()\n",
        "\n",
        "        loss_seg_record_mean = np.mean(loss_G_record)\n",
        "        loss_adv_record_mean = np.mean(loss_adv_record)\n",
        "        loss_D_record_mean = np.mean(loss_D_record)\n",
        "        loss_mean = np.mean([loss_seg_record_mean, loss_adv_record_mean, loss_D_record_mean])\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_mean), epoch)\n",
        "        print(f'loss for train :\\n - Segmentation: {loss_seg_record_mean}\\n - Adversarial: {loss_adv_record_mean}\\n - Discriminator: {loss_D_record_mean}')\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}stdc_Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM5ZhKsY83S"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYVQs2j2FGpx"
      },
      "outputs": [],
      "source": [
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        tq = tqdm(total=len(dataloader))\n",
        "        tq.set_description('Validation')\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            label = label.type(torch.LongTensor)\n",
        "            data = data.cuda()\n",
        "            label = label.long().cuda()\n",
        "\n",
        "            # get RGB predict image\n",
        "            if args.model == 'STDC-net':\n",
        "                predict, _, _ = model(data)\n",
        "            else:\n",
        "                predict = model(data)\n",
        "\n",
        "            predict = predict.squeeze(0)\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            # there is no need to transform the one-hot array to visual RGB array\n",
        "            precision_record.append(precision)\n",
        "\n",
        "            tq.update(1)\n",
        "\n",
        "        tq.close()\n",
        "        precision = np.mean(precision_record)\n",
        "        miou_list = per_class_iu(hist)\n",
        "        miou = np.mean(miou_list)\n",
        "        print('\\nprecision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        print(f'mIoU per class: {miou_list}')\n",
        "\n",
        "        return precision, miou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWJR33K6HdBA"
      },
      "source": [
        "## **Main**\n",
        "Prepare arguments, Datasets, Dataloaders, model, training and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HN2KFzdkHBs"
      },
      "source": [
        "### main definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mND4TmxJ2b1v"
      },
      "outputs": [],
      "source": [
        "def main(args, eval_only=False):\n",
        "\n",
        "    n_classes = args.num_classes\n",
        "    train_root = args.train_root\n",
        "    val_root = args.val_root\n",
        "    target_root = args.train_root\n",
        "    domain_adapt = args.domain_adaptation\n",
        "\n",
        "    # defining the training, validation (and target for domain adaptation) datasets and dataloaders\n",
        "    if train_root == 'GTA5/':\n",
        "        if train_root != val_root: # if we only use GTA5 for training, use the entire dataset and don't leave a portion for testing\n",
        "            train_dataset = GTA5(train_root, 'train', args.data_augmentation, 1)\n",
        "        else:\n",
        "            train_dataset = GTA5(train_root, 'train', args.data_augmentation)\n",
        "    else:\n",
        "        train_dataset = CityScapes(train_root, 'train')\n",
        "\n",
        "    if val_root == 'GTA5/':\n",
        "        val_dataset = GTA5(val_root, 'val')\n",
        "    else:\n",
        "        val_dataset = CityScapes(val_root, 'val')\n",
        "\n",
        "    if target_root == 'GTA5/':\n",
        "        target_dataset = GTA5(target_root, 'train')\n",
        "    else:\n",
        "        target_dataset = CityScapes(target_root, 'train')\n",
        "\n",
        "    dataloader_train = DataLoader(train_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    dataloader_val = DataLoader(val_dataset,\n",
        "                      batch_size=1,\n",
        "                      shuffle=False,\n",
        "                      num_workers=args.num_workers,\n",
        "                      drop_last=False)\n",
        "\n",
        "    dataloader_target = DataLoader(target_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    ## model\n",
        "    if args.model == 'STDC-net':\n",
        "        backbone='CatmodelSmall'\n",
        "        model = BiSeNet(backbone=backbone, n_classes=n_classes, pretrain_model=args.pretrain_path, use_conv_last=args.use_conv_last)\n",
        "    else:\n",
        "        backbone='resnet18'\n",
        "        model = BiSeNetv1(num_classes=n_classes, context_path=backbone)\n",
        "\n",
        "    ### load a saved model from start epoch\n",
        "    if args.epoch_start_i != 0:\n",
        "        print(f'loading data from saved model {args.saved_model}')\n",
        "        model.load_state_dict(torch.load(f'{args.save_model_path}{args.saved_model}'))\n",
        "\n",
        "    if torch.cuda.is_available() and args.use_gpu:\n",
        "        model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    ## optimizer\n",
        "    # build optimizer\n",
        "    if args.optimizer == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "    elif args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "    else:  # rmsprop\n",
        "        print('not supported optimizer \\n')\n",
        "        return None\n",
        "\n",
        "    if domain_adapt:\n",
        "        # init Discriminator\n",
        "        if args.discr == 'fc':\n",
        "            model_D = FCDiscriminator(args.num_classes)\n",
        "        elif args.discr == 'light':\n",
        "            model_D = LightFCDiscriminator(args.num_classes)\n",
        "        elif args.discr == 'light-thin':\n",
        "            model_D = LightLightFCDiscriminator(args.num_classes)\n",
        "\n",
        "        model_D = model_D.cuda()\n",
        "\n",
        "        if args.optimizer_D == 'rmsprop':\n",
        "            optimizer_D = torch.optim.RMSprop(model_D.parameters(), args.learning_rate_D)\n",
        "        elif args.optimizer_D == 'sgd':\n",
        "            optimizer_D = torch.optim.SGD(model_D.parameters(), args.learning_rate_D, momentum=0.9, weight_decay=1e-4)\n",
        "        elif args.optimizer_D == 'adam':\n",
        "            optimizer_D = torch.optim.Adam(model_D.parameters(), args.learning_rate_D, betas=(0.9, 0.99))\n",
        "        else:  # rmsprop\n",
        "            print('not supported optimizer \\n')\n",
        "            return None\n",
        "\n",
        "    if not eval_only: #this is for when we only care about evaluating a saved model and not about training\n",
        "        if domain_adapt:\n",
        "            ## train loop for domain adaptation\n",
        "            train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=domain_adapt, dataloader_target=dataloader_target)\n",
        "        else:\n",
        "            print(\"Train and validation on the same dataset (different partitions)\" if args.train_root==args.val_root else \"Train Domain Shift\")\n",
        "            ## normal train loop\n",
        "            train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "    # final test\n",
        "    val(args, model, dataloader_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvA6e8s5HXFJ"
      },
      "source": [
        "### main execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0wglGk1Tq63"
      },
      "source": [
        "**Tasks**:\n",
        "- Networks: model\n",
        "  - 'STDC-net' for *STDC-net*\n",
        "  - 'BiSeNetv1' for *BiSeNet v1* with ResNet-18 as backbone\n",
        "  - hyper-parameters:\n",
        "    - Learning rate: 0.001\n",
        "    - Batch size: 8\n",
        "    - Optimization algorithm: 'adam'\n",
        "- Discriminators: discr\n",
        "  - 'fc' for *FCDiscriminator*\n",
        "  - 'light' for *LightFCDiscriminator*\n",
        "  - 'light-thin' for *LightLightFCDiscriminator*\n",
        "  - hyper-parameters:\n",
        "    - Learning rate: 0.0001\n",
        "    - Optimization algorithm: 'adam'\n",
        "\n",
        "1. train on Cityscapes, validation on Cityscapes\n",
        "  - train_root='/Cityscapes/Cityspaces/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "2. train on GTA5, validation on GTA5\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/GTA5/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "3. train on GTA5, validation on GTA5 with data augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/GTA5/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=False\n",
        "4. Domain Shift: GTA5 -> Cityscapes\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "4. Domain Shift: GTA5 -> Cityscapes with data augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=False\n",
        "5. Domain Adaptation: GTA5 (source), Cityscapes (target) with training augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - target_root='/Cityscapes/Cityscapes/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=True\n",
        "  - adv_factor=0.001\n",
        "  - discr\n",
        "    - 'fc' for *FCDiscriminator*\n",
        "    - 'light' for *LightFCDiscriminator*\n",
        "    - 'light-thin' for *LightLightFCDiscriminator*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph6kWFEQFkQS",
        "outputId": "e318323e-8fed-4337-dc6c-f10bb9eb245f"
      },
      "outputs": [],
      "source": [
        "class arguments():\n",
        "    model = 'STDC-net'  #'BiSeNetv1'\n",
        "    pretrain_path = \"/content/Drive/MyDrive/Colab Notebooks/checkpoints/STDCNet813M_73.91.tar\"\n",
        "    use_conv_last = False\n",
        "    num_epochs = 50\n",
        "    epoch_start_i = 0\n",
        "    checkpoint_step = 1\n",
        "    validation_step = 5\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-3\n",
        "    learning_rate_D = 1e-4\n",
        "    optimizer = 'adam'\n",
        "    optimizer_D = 'adam'\n",
        "    num_classes = 19\n",
        "    num_workers = 2\n",
        "    cuda = '0'\n",
        "    use_gpu = True\n",
        "    save_model_path = '/content/Drive/MyDrive/Colab Notebooks/Partial models/'\n",
        "    saved_model = f'Saved_model_epoch_{epoch_start_i}.pth'  # put the name of the .pth to load\n",
        "    train_root = 'GTA5/'\n",
        "    val_root ='Cityscapes/Cityspaces/'\n",
        "    target_root = 'Cityscapes/Cityspaces/'\n",
        "    data_augmentation = True\n",
        "    domain_adaptation = True\n",
        "    discr = 'fc'  #'fc', 'light', 'light-thin'\n",
        "    adv_factor = 0.001\n",
        "main_args = arguments()\n",
        "\n",
        "main(main_args, eval_only=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PxlodwhG7hEE",
        "oD_JhI28r_mH",
        "s7Ycn47MtFEj",
        "2SXl1I5e7bJS",
        "AzUXQKPmJEI1",
        "XYmppeDwAqNY",
        "cHU8LM2IeA9T",
        "mi1cLfJGfmsb",
        "sFKadbbIGdzN",
        "p_IPp-nNYxDe",
        "y5KShRQ9Y4Gb",
        "aFM5ZhKsY83S",
        "8HN2KFzdkHBs"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
