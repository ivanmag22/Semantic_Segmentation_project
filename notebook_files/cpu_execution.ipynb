{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxlodwhG7hEE"
      },
      "source": [
        "## **Installing packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sINdgFRi71jI",
        "outputId": "c542f0c2-a5ce-4047-8061-ceb53521a041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio torchsummary\n",
        "!pip install 'tqdm'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dIkbogrr3LY",
        "outputId": "7fd1e005-b292-4162-970f-45144fe59c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.25.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JnWbLMOA_v",
        "outputId": "1db9b75b-0f60-4ac7-ee2d-1645a714aea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m868.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath>=0.1.7->fvcore) (4.12.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=705334774a1a7cf03815186869d35c21349f1ab9b79ca914d6701e3e75dad43b\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=41e2bc8d7f7cfa475ea186f1a1ff9ed6d3fcc9b22d76d06e452c71109882d042\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install -U fvcore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_JhI28r_mH"
      },
      "source": [
        "## **Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xBUFnC9vr-FZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader, Dataset\n",
        "from torch.backends import cudnn\n",
        "import torch.cuda.amp as amp\n",
        "import torch.cpu.amp as amp_cpu\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "from torchsummary import summary\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "\n",
        "logger = logging.getLogger()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7Ycn47MtFEj"
      },
      "source": [
        "## **Import**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SXl1I5e7bJS"
      },
      "source": [
        "### Importing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIh7yVFKOlXF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/Drive')\n",
        "\n",
        "cityscapes = True\n",
        "gta5 = True\n",
        "\n",
        "if not os.path.isdir(f'/content/Cityscapes') and cityscapes:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/Cityscapes.zip\"\n",
        "if not os.path.isdir(f'/content/GTA5') and gta5:\n",
        "  !jar xvf  \"/content/Drive/MyDrive/Colab Notebooks/AML/GTA5.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzUXQKPmJEI1"
      },
      "source": [
        "### Import methods from TA's repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CNz4CFTuY6a",
        "outputId": "a650d952-ef27-4fa8-c89d-a7f649e5699c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'AML_Semantic_DA'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 9\u001b[K\n",
            "Receiving objects: 100% (18/18), 10.88 KiB | 5.44 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# cloning github repo for model (BiseNet with STDC) and utils, I rewrote manually the Train.py and Cityscapes.py below\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/ClaudiaCuttano/AML_Semantic_DA.git\n",
        "\n",
        "\n",
        "# importing stuff from the repo we just cloned\n",
        "\n",
        "# copied from train.py\n",
        "from AML_Semantic_DA.model.model_stages import BiSeNet  # see https://github.com/ClaudiaCuttano/AML_Semantic_DA/blob/master/model/model_stages.py\n",
        "from AML_Semantic_DA.utils import poly_lr_scheduler, reverse_one_hot, compute_global_accuracy, fast_hist, per_class_iu # see https://github.com/ClaudiaCuttano/AML_Semantic_DA/blob/master/utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYmppeDwAqNY"
      },
      "source": [
        "### Import methods from our repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lh2kJl2AqNf",
        "outputId": "e5876b99-c8c8-43e8-bcb4-39a40d8c9c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "fatal: destination path 'Semantic_Segmentation_project' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# cloning github repo for model (BiseNet with STDC) and utils, I rewrote manually the Train.py and Cityscapes.py below\n",
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/ivanmag22/Semantic_Segmentation_project.git\n",
        "\n",
        "\n",
        "# importing stuff from the repo we just cloned\n",
        "from Semantic_Segmentation_project.model.model_stages import BiSeNet\n",
        "from Semantic_Segmentation_project.model.bisenetv1 import BiSeNet as BiSeNetv1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHU8LM2IeA9T"
      },
      "source": [
        "### Import methods for GTA5\n",
        "Clone repo useful to give a label for each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS3OpszpeCFq",
        "outputId": "073c8c46-0af3-4fa7-d23b-84e480913f2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'STDC_seg'...\n",
            "remote: Enumerating objects: 98, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 98 (delta 18), reused 8 (delta 8), pack-reused 70\u001b[K\n",
            "Receiving objects: 100% (98/98), 1.93 MiB | 7.27 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "print(pathlib.Path.cwd())\n",
        "!git clone https://github.com/MichaelFan01/STDC-Seg.git STDC_seg\n",
        "# importing stuff from the repo we just cloned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi1cLfJGfmsb"
      },
      "source": [
        "## **Data loader classes + Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdWc-Bs6tSBG"
      },
      "source": [
        "### Data Pre Processing\n",
        "We need to do pre-processing only on **training set** (and not on validation set because on the last one we work with 1024 x 2048 pictures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fg5N6jHd5dCy"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                      transforms.ToTensor()\n",
        "                                      ])\n",
        "label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "eval_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "gta_train_transform = transforms.Compose([transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR),\n",
        "                                          transforms.ToTensor()\n",
        "                                          ])\n",
        "gta_label_transform = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST)\n",
        "gta_val_transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "\n",
        "#data augmentation\n",
        "bright_t = transforms.ColorJitter(brightness=[1,2])\n",
        "contrast_t = transforms.ColorJitter(contrast = [2,5])\n",
        "saturation_t = transforms.ColorJitter(saturation = [1,3])\n",
        "hue_t = transforms.ColorJitter(hue = 0.2)\n",
        "gs_t = transforms.Grayscale(3)\n",
        "hflip_t = transforms.RandomHorizontalFlip(p = 1)\n",
        "cc_t = transforms.CenterCrop((256,512))\n",
        "\n",
        "augmentation_transforms = [bright_t, contrast_t, saturation_t, hue_t, gs_t, hflip_t, cc_t]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CcJLDmjscHZ"
      },
      "source": [
        "### Cityscapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MrzZRbQsFeJq"
      },
      "outputs": [],
      "source": [
        "class CityScapes(Dataset):\n",
        "    def __init__(self, base_root, mode):\n",
        "        super(CityScapes, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.mask_paths_colored = [] # colored images\n",
        "        self.mask_paths_bw = [] # labels\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "        image_folder = f'{base_root}images/{mode}'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                mask_path_bw = image_path.replace('leftImg8bit', 'gtFine_labelTrainIds')\n",
        "                mask_path_bw = mask_path_bw.replace('/images/', '/gtFine/')\n",
        "                assert(Path(mask_path_bw).is_file())\n",
        "                self.mask_paths_bw.append(mask_path_bw)\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            self.image_transform = train_transform\n",
        "        if self.mode == 'val':\n",
        "            self.image_transform = eval_transform\n",
        "        self.label_transform = label_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.mask_paths_bw))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        label = Image.open(self.mask_paths_bw[index])\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            label = np.array(self.label_transform(label))[np.newaxis, :]\n",
        "        else:\n",
        "            label = np.array(label)[np.newaxis, :]\n",
        "\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXcCX6cTyyBi"
      },
      "source": [
        "### GTA5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s2VtAICsy0QY"
      },
      "outputs": [],
      "source": [
        "class GTA5(Dataset):\n",
        "    def __init__(self, base_root, mode, augmentation=False, train_test_rateo=2/3):\n",
        "        super(GTA5, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "        self.image_paths = [] # images\n",
        "        self.label_paths = [] # labels\n",
        "        self.train_test_rateo = train_test_rateo\n",
        "        self.augmentation = augmentation\n",
        "        with open('STDC_seg/cityscapes_info.json', 'r') as fr:\n",
        "            labels_info = json.load(fr)\n",
        "        self.label_map = {el['id']: el['trainId'] for el in labels_info}\n",
        "        self.label_map.update({34 : 255})\n",
        "\n",
        "        assert(mode == 'train' or mode == 'val')  # just checking for potential issues\n",
        "\n",
        "        image_folder = f'{base_root}images'\n",
        "\n",
        "        for root, dirs, files in os.walk(image_folder):\n",
        "            for file_name in files:\n",
        "                image_path = f'{root}/{file_name}'\n",
        "                assert(Path(image_path).is_file())\n",
        "                self.image_paths.append(image_path)\n",
        "\n",
        "                label_path = image_path.replace('/images/', '/labels/')\n",
        "                assert(Path(label_path).is_file())\n",
        "                self.label_paths.append(label_path)\n",
        "\n",
        "        l = int(len(self.image_paths) * self.train_test_rateo)\n",
        "        if self.mode == 'train':\n",
        "            self.image_paths = self.image_paths[:l]\n",
        "            self.label_paths = self.label_paths[:l]\n",
        "            self.image_transform = gta_train_transform\n",
        "            self.label_transform = gta_label_transform\n",
        "        elif self.mode == 'val':\n",
        "            self.image_paths = self.image_paths[l:]\n",
        "            self.label_paths = self.label_paths[l:]\n",
        "            self.image_transform = gta_val_transform\n",
        "\n",
        "        assert (len(self.image_paths) != 0)\n",
        "        assert (len(self.image_paths) == len(self.label_paths))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        image = Image.open(self.image_paths[index]).convert('RGB')\n",
        "        image = self.image_transform(image)\n",
        "\n",
        "        label = Image.open(self.label_paths[index])\n",
        "        if self.mode == 'train':\n",
        "            label = self.label_transform(label)\n",
        "\n",
        "        if self.augmentation and random.choice([True, False]) and self.mode == 'train':\n",
        "            idx = random.randint(0, 6)\n",
        "\n",
        "            image = augmentation_transforms[idx](image)\n",
        "\n",
        "            if hflip_t is augmentation_transforms[idx] or cc_t is augmentation_transforms[idx]:\n",
        "                label = augmentation_transforms[idx](label)\n",
        "\n",
        "            if cc_t is augmentation_transforms[idx]:\n",
        "                rimage_t = transforms.Resize((512,1024), transforms.InterpolationMode.BILINEAR, antialias=None)\n",
        "                rlabel_t = transforms.Resize((512,1024), transforms.InterpolationMode.NEAREST, antialias=None)\n",
        "                image = rimage_t(image)\n",
        "                label = augmentation_transforms[idx](label)\n",
        "                label = rlabel_t(label)\n",
        "\n",
        "        label = np.array(label).astype(np.int64)[np.newaxis, :]\n",
        "        label = self.convert_labels(label)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "    def convert_labels(self, label):\n",
        "        for k, v in self.label_map.items():\n",
        "            label[label == k] = v\n",
        "        return label\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFKadbbIGdzN"
      },
      "source": [
        "## **Architecture**\n",
        "BiSeNet as Domain Adaptation Neural Network (DANN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ORORR6UuGmkv"
      },
      "outputs": [],
      "source": [
        "class DepthWiseSeparableConvolution(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(DepthWiseSeparableConvolution, self).__init__()\n",
        "        self.depth_wise = nn.Conv2d(ch_in, ch_in, kernel_size=4, stride=2, padding=1, groups=ch_in)\n",
        "        self.point_wise = nn.Conv2d(ch_in, ch_out, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.depth_wise(x)\n",
        "        out = self.point_wise(out)\n",
        "        return out\n",
        "\n",
        "class LightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.conv3 = DepthWiseSeparableConvolution(ndf*2, ndf*4)\n",
        "        self.conv4 = DepthWiseSeparableConvolution(ndf*4, ndf*8)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*8, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class LightLightFCDiscriminator(nn.Module):\n",
        "    def __init__(self, num_classes, ndf=64):\n",
        "        super(LightLightFCDiscriminator, self).__init__()\n",
        "\n",
        "        # context\n",
        "        self.conv1 = DepthWiseSeparableConvolution(num_classes, ndf)\n",
        "        self.conv2 = DepthWiseSeparableConvolution(ndf, ndf*2)\n",
        "        self.classifier = DepthWiseSeparableConvolution(ndf*2, 1)\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=8, mode='bilinear')\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        return x\n",
        "\n",
        "class FCDiscriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, ndf = 64):\n",
        "        super(FCDiscriminator, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
        "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "        self.up_sample = nn.Upsample(scale_factor=32, mode='bilinear')\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.classifier(x)\n",
        "        x = self.up_sample(x)\n",
        "        #x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIHcQMaMG_o9"
      },
      "source": [
        "## **Train + Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_IPp-nNYxDe"
      },
      "source": [
        "### Base Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "e0DE8M0pFGhy"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train(args, model, optimizer, dataloader_train, dataloader_val):\n",
        "    print(\"start train without domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/AML project/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    loss_func = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch, max_iter=args.num_epochs)\n",
        "        model.train() # Sets module in training mode\n",
        "        tq = tqdm(total=len(dataloader_train) * args.batch_size)\n",
        "        tq.set_description('epoch %d, lr %f' % (epoch, lr))\n",
        "        loss_record = []\n",
        "\n",
        "        for i, (data, label) in enumerate(dataloader_train):\n",
        "            label = label.long()\n",
        "\n",
        "            optimizer.zero_grad() # Zero-ing the gradients\n",
        "\n",
        "\n",
        "            output, out16, out32 = model(data)  # Forward pass to the network\n",
        "            # Compute loss based on output and ground truth\n",
        "            loss1 = loss_func(output, label.squeeze(1))\n",
        "            loss2 = loss_func(out16, label.squeeze(1))\n",
        "            loss3 = loss_func(out32, label.squeeze(1))\n",
        "            loss = loss1 + loss2 + loss3  # sum of losses\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            tq.set_postfix(loss='%.6f' % loss)\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_step', loss, step)\n",
        "            loss_record.append(loss.item())\n",
        "        tq.close()\n",
        "        loss_train_mean = np.mean(loss_record)\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_train_mean), epoch)\n",
        "        print('loss for train : %f' % (loss_train_mean))\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5KShRQ9Y4Gb"
      },
      "source": [
        "### Domain Adaptation Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "fSs7A9BOXTDS"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=False, dataloader_target=None):\n",
        "    print(\"start train with domain adaptation\")\n",
        "    # for SummaryWriter read (https://pytorch.org/docs/stable/tensorboard.html)\n",
        "    writer = SummaryWriter(log_dir='/content/Drive/MyDrive/AML project/logs', comment=''.format(args.optimizer)) # log is in run/ folder\n",
        "\n",
        "    loss_func_G = torch.nn.CrossEntropyLoss(ignore_index=255)\n",
        "    loss_func_adv = torch.nn.BCEWithLogitsLoss()\n",
        "    loss_func_D = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    max_miou = 0\n",
        "    step = 0\n",
        "\n",
        "    # see (https://www.github.com/wasidennis/AdaptSegNet/blob/master/train_gta2cityscapes_multi)\n",
        "    print(\"Train DA\")\n",
        "\n",
        "    LAMBDA_ADV_TARGET = args.adv_factor\n",
        "\n",
        "    dataloader_len = min(len(dataloader_train), len(dataloader_target))\n",
        "    for epoch in range(args.epoch_start_i+1, args.num_epochs+1):\n",
        "        lr = poly_lr_scheduler(optimizer, args.learning_rate, iter=epoch-1, max_iter=args.num_epochs)\n",
        "        lr_D = poly_lr_scheduler(optimizer_D, args.learning_rate_D, iter=epoch-1, max_iter=args.num_epochs)\n",
        "\n",
        "        model.train()\n",
        "        model_D.train()\n",
        "\n",
        "        tq = tqdm(total=dataloader_len * args.batch_size)\n",
        "\n",
        "        tq.set_description('epoch %d, lr %f, lr_discriminator %f' % (epoch, lr, lr_D))\n",
        "\n",
        "        # set the ground truth for the discriminator\n",
        "        source_label = 0\n",
        "        target_label = 1\n",
        "        # initiate lists to track the losses\n",
        "        loss_G_record = []                                                       # track the Segmentation loss\n",
        "        loss_adv_record = []                                                     # track the advarsirial loss\n",
        "        loss_D_record = []                                                       # track the discriminator loss\n",
        "\n",
        "        source_train_loader = enumerate(dataloader_train)\n",
        "        s_size = len(dataloader_train)\n",
        "        target_loader = enumerate(dataloader_target)\n",
        "        t_size = len(dataloader_target)\n",
        "\n",
        "        for i in range(dataloader_len):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            optimizer_D.zero_grad()\n",
        "\n",
        "            # =====================================\n",
        "            # train Generator G:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(source_train_loader)\n",
        "            data, label = batch\n",
        "            label = label.long()\n",
        "\n",
        "            output_s, out16, out32 = model(data)\n",
        "            loss1 = loss_func_G(output_s, label.squeeze(1))\n",
        "            loss2 = loss_func_G(out16, label.squeeze(1))\n",
        "            loss3 = loss_func_G(out32, label.squeeze(1))\n",
        "            loss_G = loss1 + loss2 + loss3\n",
        "\n",
        "            loss_G.backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            _, batch = next(target_loader)\n",
        "\n",
        "            data, _ = batch\n",
        "\n",
        "            output_t, _, _ = model(data)\n",
        "            D_out = model_D(F.softmax(output_t, dim=1))\n",
        "            loss_adv = loss_func_adv(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)))\n",
        "            loss_adv = loss_adv * LAMBDA_ADV_TARGET\n",
        "\n",
        "            loss_adv.backward()\n",
        "\n",
        "            # =====================================\n",
        "            # train Discriminator D:\n",
        "            # =====================================\n",
        "\n",
        "            for param in model_D.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "            # Train with source:\n",
        "            # =================================\n",
        "\n",
        "            output_s = output_s.detach()\n",
        "\n",
        "            D_out = model_D(F.softmax(output_s, dim=1))                                                                   # we feed the discriminator with the output of the G-model\n",
        "            loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(source_label)))\n",
        "\n",
        "            loss_D.backward()\n",
        "\n",
        "            # Train with target:\n",
        "            # =================================\n",
        "\n",
        "            output_t = output_t.detach()\n",
        "            D_out = model_D(F.softmax(output_t, dim=1))  # we feed the discriminator with the output of the model\n",
        "            loss_D = loss_func_D(D_out, Variable(torch.FloatTensor(D_out.data.size()).fill_(target_label)))\n",
        "\n",
        "            tq.update(args.batch_size)\n",
        "            losses = {\"loss_seg\" : '%.6f' %(loss_G.item())  , \"loss_adv\" : '%.6f' %(loss_adv.item()) , \"loss_D\" : '%.6f'%(loss_D.item()) } # add dictionary to print losses\n",
        "            tq.set_postfix(losses)\n",
        "\n",
        "            loss_G_record.append(loss_G.item())\n",
        "            loss_adv_record.append(loss_adv.item())\n",
        "            loss_D_record.append(loss_D.item())\n",
        "            step += 1\n",
        "            writer.add_scalar('loss_G_step', loss_G, step)  # track the segmentation loss\n",
        "            writer.add_scalar('loss_adv_step', loss_adv, step)  # track the adversarial loss\n",
        "            writer.add_scalar('loss_D_step', loss_D, step)  # track the discreminator loss\n",
        "            optimizer.step()  # update the optimizer for genarator\n",
        "            optimizer_D.step()  # update the optimizer for discriminator\n",
        "        tq.close()\n",
        "\n",
        "        loss_seg_record_mean = np.mean(loss_G_record)\n",
        "        loss_adv_record_mean = np.mean(loss_adv_record)\n",
        "        loss_D_record_mean = np.mean(loss_D_record)\n",
        "        loss_mean = np.mean([loss_seg_record_mean, loss_adv_record_mean, loss_D_record_mean])\n",
        "        writer.add_scalar('epoch/loss_epoch_train', float(loss_mean), epoch)\n",
        "        print(f'loss for train :\\n - Segmentation: {loss_seg_record_mean}\\n - Adversarial: {loss_adv_record_mean}\\n - Discriminator: {loss_D_record_mean}')\n",
        "\n",
        "        if epoch % args.checkpoint_step == 0 and epoch != 0:\n",
        "            import os\n",
        "            if not os.path.isdir(args.save_model_path):\n",
        "                os.mkdir(args.save_model_path)\n",
        "            torch.save(model.module.state_dict(), f'{args.save_model_path}cpu_Saved_model_epoch_{epoch}.pth')\n",
        "\n",
        "        if epoch % args.validation_step == 0 and epoch != args.num_epochs:\n",
        "            precision, miou = val(args, model, dataloader_val)  # val() function call\n",
        "            if miou > max_miou:\n",
        "                max_miou = miou\n",
        "                import os\n",
        "                os.makedirs(args.save_model_path, exist_ok=True)\n",
        "                torch.save(model.module.state_dict(), f'{args.save_model_path}cpu_Best_model_epoch_{epoch}.pth')\n",
        "            writer.add_scalar('epoch/precision_val', precision, epoch)\n",
        "            writer.add_scalar('epoch/miou val', miou, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFM5ZhKsY83S"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xYVQs2j2FGpx"
      },
      "outputs": [],
      "source": [
        "def val(args, model, dataloader):\n",
        "    print('start val!')\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        precision_record = []\n",
        "        hist = np.zeros((args.num_classes, args.num_classes))\n",
        "        tq = tqdm(total=len(dataloader))\n",
        "        tq.set_description('Validation')\n",
        "        for i, (data, label) in enumerate(dataloader):\n",
        "            label = label.type(torch.LongTensor)\n",
        "            label.long()\n",
        "\n",
        "            # get RGB predict image\n",
        "            if args.model == 'STDC-net':\n",
        "                predict, _, _ = model(data)\n",
        "            else:\n",
        "                predict = model(data)\n",
        "\n",
        "            predict = predict.squeeze(0)\n",
        "            predict = reverse_one_hot(predict)\n",
        "            predict = np.array(predict.cpu())\n",
        "\n",
        "            # get RGB label image\n",
        "            label = label.squeeze()\n",
        "            label = np.array(label.cpu())\n",
        "\n",
        "            # compute per pixel accuracy\n",
        "            precision = compute_global_accuracy(predict, label)\n",
        "            hist += fast_hist(label.flatten(), predict.flatten(), args.num_classes)\n",
        "\n",
        "            # there is no need to transform the one-hot array to visual RGB array\n",
        "            precision_record.append(precision)\n",
        "\n",
        "            tq.update(1)\n",
        "\n",
        "        tq.close()\n",
        "        precision = np.mean(precision_record)\n",
        "        miou_list = per_class_iu(hist)\n",
        "        miou = np.mean(miou_list)\n",
        "        print('\\nprecision per pixel for test: %.3f' % precision)\n",
        "        print('mIoU for validation: %.3f' % miou)\n",
        "        print(f'mIoU per class: {miou_list}')\n",
        "\n",
        "        return precision, miou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHqHYhrK1cHU"
      },
      "source": [
        "## **Main**\n",
        "Prepare arguments, Datasets, Dataloaders, model, training and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8kp6Y8w1cHU"
      },
      "source": [
        "### main definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-ZUw1hD41cHU"
      },
      "outputs": [],
      "source": [
        "def main(args, eval_only=False):\n",
        "\n",
        "    n_classes = args.num_classes\n",
        "    train_root = args.train_root\n",
        "    val_root = args.val_root\n",
        "    target_root = args.train_root\n",
        "    domain_adapt = args.domain_adaptation\n",
        "\n",
        "    # defining the training, validation (and target for domain adaptation) datasets and dataloaders\n",
        "    if train_root == 'GTA5/':\n",
        "        if train_root != val_root: # if we only use GTA5 for training, use the entire dataset and don't leave a portion for testing\n",
        "            train_dataset = GTA5(train_root, 'train', args.data_augmentation, 1)\n",
        "        else:\n",
        "            train_dataset = GTA5(train_root, 'train', args.data_augmentation)\n",
        "    else:\n",
        "        train_dataset = CityScapes(train_root, 'train')\n",
        "\n",
        "    if val_root == 'GTA5/':\n",
        "        val_dataset = GTA5(val_root, 'val')\n",
        "    else:\n",
        "        val_dataset = CityScapes(val_root, 'val')\n",
        "\n",
        "    if target_root == 'GTA5/':\n",
        "        target_dataset = GTA5(target_root, 'train')\n",
        "    else:\n",
        "        target_dataset = CityScapes(target_root, 'train')\n",
        "\n",
        "    dataloader_train = DataLoader(train_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    dataloader_val = DataLoader(val_dataset,\n",
        "                      batch_size=1,\n",
        "                      shuffle=False,\n",
        "                      num_workers=args.num_workers,\n",
        "                      drop_last=False)\n",
        "\n",
        "    dataloader_target = DataLoader(target_dataset,\n",
        "                      batch_size=args.batch_size,\n",
        "                      shuffle=True,\n",
        "                      num_workers=args.num_workers,\n",
        "                      pin_memory=False,\n",
        "                      drop_last=True)\n",
        "\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "    ## model\n",
        "    if args.model == 'STDC-net':\n",
        "        backbone='CatmodelSmall'\n",
        "        model = BiSeNet(backbone=backbone, n_classes=n_classes, pretrain_model=args.pretrain_path, use_conv_last=args.use_conv_last, device=device)\n",
        "    else:\n",
        "        backbone='resnet18'\n",
        "        model = BiSeNetv1(num_classes=n_classes, context_path=backbone)\n",
        "\n",
        "    ### load a saved model from start epoch\n",
        "    if args.epoch_start_i != 0:\n",
        "        print(f'loading data from saved model {args.saved_model}')\n",
        "        model.load_state_dict(torch.load(f'{args.save_model_path}{args.saved_model}'))\n",
        "\n",
        "    ## optimizer\n",
        "    # build optimizer\n",
        "    if args.optimizer == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), args.learning_rate)\n",
        "    elif args.optimizer == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), args.learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "    elif args.optimizer == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), args.learning_rate)\n",
        "    else:  # rmsprop\n",
        "        print('not supported optimizer \\n')\n",
        "        return None\n",
        "\n",
        "    if domain_adapt:\n",
        "        # init Discriminator\n",
        "        if args.discr == 'fc':\n",
        "            model_D = FCDiscriminator(args.num_classes)\n",
        "        elif args.discr == 'light':\n",
        "            model_D = LightFCDiscriminator(args.num_classes)\n",
        "        elif args.discr == 'light-thin':\n",
        "            model_D = LightLightFCDiscriminator(args.num_classes)\n",
        "\n",
        "        if args.optimizer_D == 'rmsprop':\n",
        "            optimizer_D = torch.optim.RMSprop(model_D.parameters(), args.learning_rate_D)\n",
        "        elif args.optimizer_D == 'sgd':\n",
        "            optimizer_D = torch.optim.SGD(model_D.parameters(), args.learning_rate_D, momentum=0.9, weight_decay=1e-4)\n",
        "        elif args.optimizer_D == 'adam':\n",
        "            optimizer_D = torch.optim.Adam(model_D.parameters(), args.learning_rate_D, betas=(0.9, 0.99))\n",
        "        else:  # rmsprop\n",
        "            print('not supported optimizer \\n')\n",
        "            return None\n",
        "\n",
        "    if not eval_only: #this is for when we only care about evaluating a saved model and not about training\n",
        "        if domain_adapt:\n",
        "            ## train loop for domain adaptation\n",
        "            train_da(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt=domain_adapt, dataloader_target=dataloader_target)\n",
        "        else:\n",
        "            print(\"Train and validation on the same dataset (different partitions)\" if args.train_root==args.val_root else \"Train Domain Shift\")\n",
        "            ## normal train loop\n",
        "            train(args, model, optimizer, dataloader_train, dataloader_val)\n",
        "    # final test\n",
        "    val(args, model, dataloader_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5znLdI6z1cHV"
      },
      "source": [
        "### main execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_SvouSi1cHV"
      },
      "source": [
        "**Tasks**:\n",
        "- Networks: model\n",
        "  - 'STDC-net' for *STDC-net*\n",
        "  - 'BiSeNetv1' for *BiSeNet v1* with ResNet-18 as backbone\n",
        "1. train on Cityscapes, validation on Cityscapes\n",
        "  - train_root='/Cityscapes/Cityspaces/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "2. train on GTA5, validation on GTA5\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/GTA5/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "3. train on GTA5, validation on GTA5 with data augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/GTA5/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=False\n",
        "4. Domain Shift: GTA5 -> Cityscapes\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=False\n",
        "  - domain_adaptation=False\n",
        "4. Domain Shift: GTA5 -> Cityscapes with data augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=False\n",
        "5. Domain Adaptation: GTA5 (source), Cityscapes (target) with training augmentation\n",
        "  - train_root='/GTA5/'\n",
        "  - val_root='/Cityscapes/Cityspaces/'\n",
        "  - target_root='/Cityscapes/Cityscapes/'\n",
        "  - data_augmentation=True\n",
        "  - domain_adaptation=True\n",
        "  - adv_factor=0.001\n",
        "  - discr\n",
        "    - 'fc' for *FCDiscriminator*\n",
        "    - 'light' for *LightFCDiscriminator*\n",
        "    - 'light-thin' for *LightLightFCDiscriminator*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "k0IOAe4Z1cHV",
        "outputId": "6a5ff689-acc5-4d8d-d99e-6ed12a5dd716"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use pretrain model /content/Drive/MyDrive/Colab Notebooks/checkpoints/STDCNet813M_73.91.tar\n",
            "start train with domain adaptation\n",
            "Train DA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1, lr 0.001000, lr_discriminator 0.000100:   1%|          | 16/1664 [04:48<8:06:24, 17.71s/it, loss_seg=8.129649, loss_adv=0.000673, loss_D=0.713733] "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ede2ac0be37a>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-9b270e51d783>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args, eval_only)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdomain_adapt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m## train loop for domain adaptation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mtrain_da\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain_adapt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain_adapt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataloader_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train and validation on the same dataset (different partitions)\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_root\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_root\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"Train Domain Shift\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-16c3a88e94e3>\u001b[0m in \u001b[0;36mtrain_da\u001b[0;34m(args, model, optimizer, model_D, optimizer_D, dataloader_train, dataloader_val, domain_adapt, dataloader_target)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0moutput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mD_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mloss_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func_adv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Semantic_Segmentation_project/model/model_stages.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mfeat_fuse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_res8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat_cp8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mfeat_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_fuse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mfeat_out16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_cp8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mfeat_out32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_cp16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Semantic_Segmentation_project/model/model_stages.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Semantic_Segmentation_project/model/model_stages.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "class arguments():\n",
        "    model = 'STDC-net'  #'BiSeNetv1'\n",
        "    pretrain_path = \"/content/Drive/MyDrive/Colab Notebooks/checkpoints/STDCNet813M_73.91.tar\"\n",
        "    use_conv_last = False\n",
        "    num_epochs = 1  #50\n",
        "    epoch_start_i = 0\n",
        "    checkpoint_step = 1\n",
        "    validation_step = 1 #5\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-3\n",
        "    learning_rate_D = 1e-4\n",
        "    optimizer = 'adam'\n",
        "    optimizer_D = 'adam'\n",
        "    num_classes = 19\n",
        "    num_workers = 2\n",
        "    save_model_path = '/content/Drive/MyDrive/Colab Notebooks/Partial models/'\n",
        "    saved_model = f'cpu_Saved_model_epoch_{epoch_start_i}.pth'  # put the name of the .pth to load\n",
        "    train_root = 'GTA5/'\n",
        "    val_root ='Cityscapes/Cityspaces/'\n",
        "    target_root = 'Cityscapes/Cityspaces/'\n",
        "    data_augmentation = True\n",
        "    domain_adaptation = True\n",
        "    discr = 'fc'  #'fc', 'light', 'light-thin'\n",
        "    adv_factor = 0.001\n",
        "main_args = arguments()\n",
        "\n",
        "main(main_args, eval_only=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "AzUXQKPmJEI1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}